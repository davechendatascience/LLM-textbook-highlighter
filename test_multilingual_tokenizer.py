#!/usr/bin/env python3
"""
Test Multilingual Tokenizer
Tests the multilingual tokenizer functionality for various languages
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.utils.multilingual_tokenizer import MultilingualTokenizer, get_tokenizer
import tiktoken


def test_basic_tokenizer():
    """Test basic tokenizer functionality"""
    print("ğŸ§ª Testing basic tokenizer functionality...")
    
    # Test multilingual tokenizer
    multi_tokenizer = MultilingualTokenizer()
    tiktoken_tokenizer = tiktoken.get_encoding("cl100k_base")
    
    # Test English text
    english_text = "This is a test of the multilingual tokenizer."
    multi_tokens = multi_tokenizer.encode(english_text)
    tiktoken_tokens = tiktoken_tokenizer.encode(english_text)
    
    print(f"English text: '{english_text}'")
    print(f"Multilingual tokens: {len(multi_tokens)} tokens")
    print(f"Tiktoken tokens: {len(tiktoken_tokens)} tokens")
    print(f"Token count match: {len(multi_tokens) == len(tiktoken_tokens)}")
    print()


def test_chinese_tokenizer():
    """Test Chinese text tokenization"""
    print("ğŸ§ª Testing Chinese text tokenization...")
    
    multi_tokenizer = MultilingualTokenizer()
    
    # Test Chinese text
    chinese_text = "é€™æ˜¯ä¸€å€‹å¤šèªè¨€æ¨™è¨˜å™¨çš„æ¸¬è©¦ã€‚æˆ‘å€‘éœ€è¦ç¢ºä¿ä¸­æ–‡æ–‡æœ¬èƒ½å¤ æ­£ç¢ºè™•ç†ã€‚"
    tokens = multi_tokenizer.encode(chinese_text)
    
    print(f"Chinese text: '{chinese_text}'")
    print(f"Character count: {len(chinese_text)}")
    print(f"Token count: {len(tokens)}")
    print(f"Tokens: {tokens[:10]}...")  # Show first 10 tokens
    
    # Test decoding
    decoded_text = multi_tokenizer.decode(tokens)
    print(f"Decoded text: '{decoded_text}'")
    print(f"Decoding successful: {decoded_text == chinese_text}")
    print()


def test_mixed_language_detection():
    """Test mixed language detection"""
    print("ğŸ§ª Testing mixed language detection...")
    
    multi_tokenizer = MultilingualTokenizer()
    
    # Test mixed Chinese-English text
    mixed_text = "This is a mixed ä¸­æ–‡ and English æ–‡æœ¬ test."
    
    is_mixed = multi_tokenizer.is_mixed_language(mixed_text)
    detected_lang = multi_tokenizer.detect_language_robust(mixed_text)
    
    print(f"Mixed text: '{mixed_text}'")
    print(f"Is mixed language: {is_mixed}")
    print(f"Detected language: {detected_lang}")
    print()


def test_chunking_functionality():
    """Test text chunking functionality"""
    print("ğŸ§ª Testing text chunking functionality...")
    
    multi_tokenizer = MultilingualTokenizer()
    
    # Test English chunking
    english_text = "This is a longer text that should be chunked into smaller pieces. " * 20
    english_chunks = multi_tokenizer.chunk_text(english_text, max_tokens=50, overlap=10)
    
    print(f"English text length: {len(english_text)} characters")
    print(f"English chunks: {len(english_chunks)} chunks")
    for i, chunk in enumerate(english_chunks[:3]):  # Show first 3 chunks
        print(f"  Chunk {i+1}: '{chunk[:50]}...'")
    
    # Test Chinese chunking
    chinese_text = "é€™æ˜¯ä¸€å€‹è¼ƒé•·çš„æ–‡æœ¬ï¼Œæ‡‰è©²è¢«åˆ†å‰²æˆè¼ƒå°çš„ç‰‡æ®µã€‚" * 20
    chinese_chunks = multi_tokenizer.chunk_text(chinese_text, max_tokens=50, overlap=10)
    
    print(f"\nChinese text length: {len(chinese_text)} characters")
    print(f"Chinese chunks: {len(chinese_chunks)} chunks")
    for i, chunk in enumerate(chinese_chunks[:3]):  # Show first 3 chunks
        print(f"  Chunk {i+1}: '{chunk[:20]}...'")
    print()


def test_vector_store_integration():
    """Test integration with vector store chunking"""
    print("ğŸ§ª Testing vector store integration...")
    
    from src.services.vector_store import VectorStoreService
    
    # Test with multilingual tokenizer
    vector_store_multi = VectorStoreService(use_multilingual_tokenizer=True)
    
    # Test with standard tokenizer
    vector_store_standard = VectorStoreService(use_multilingual_tokenizer=False)
    
    # Test text
    test_text = "This is a test document with some Chinese content: é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æª”ã€‚"
    
    print(f"Test text: '{test_text}'")
    
    # Compare chunking
    multi_chunks = vector_store_multi.chunk_text(test_text, max_tokens=100, overlap=20)
    standard_chunks = vector_store_standard.chunk_text(test_text, max_tokens=100, overlap=20)
    
    print(f"Multilingual chunks: {len(multi_chunks)}")
    print(f"Standard chunks: {len(standard_chunks)}")
    
    for i, (multi_chunk, standard_chunk) in enumerate(zip(multi_chunks, standard_chunks)):
        print(f"Chunk {i+1}:")
        print(f"  Multilingual: '{multi_chunk[:50]}...'")
        print(f"  Standard: '{standard_chunk[:50]}...'")
        print()
    print()


def test_context_retrieval_simulation():
    """Simulate context retrieval to test if chunks are appropriate"""
    print("ğŸ§ª Testing context retrieval simulation...")
    
    from src.services.vector_store import VectorStoreService
    
    # Create a test document with mixed content
    test_document = """
    Introduction to Machine Learning
    
    Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions without being explicitly programmed.
    
    æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹å­é›†ï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿèƒ½å¤ å­¸ç¿’ä¸¦åšå‡ºæ±ºç­–ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚
    
    Key Concepts:
    1. Supervised Learning: Learning from labeled data
    2. Unsupervised Learning: Finding patterns in unlabeled data
    3. Reinforcement Learning: Learning through interaction with environment
    
    ä¸»è¦æ¦‚å¿µï¼š
    1. ç›£ç£å­¸ç¿’ï¼šå¾æ¨™è¨˜æ•¸æ“šä¸­å­¸ç¿’
    2. ç„¡ç›£ç£å­¸ç¿’ï¼šåœ¨æœªæ¨™è¨˜æ•¸æ“šä¸­å°‹æ‰¾æ¨¡å¼
    3. å¼·åŒ–å­¸ç¿’ï¼šé€šéèˆ‡ç’°å¢ƒäº’å‹•ä¾†å­¸ç¿’
    
    Applications:
    - Natural Language Processing
    - Computer Vision
    - Recommendation Systems
    
    æ‡‰ç”¨ï¼š
    - è‡ªç„¶èªè¨€è™•ç†
    - è¨ˆç®—æ©Ÿè¦–è¦º
    - æ¨è–¦ç³»çµ±
    """
    
    # Test with multilingual tokenizer
    vector_store = VectorStoreService(use_multilingual_tokenizer=True)
    
    # Chunk the document
    chunks = vector_store.chunk_text(test_document, max_tokens=150, overlap=30)
    
    print(f"Document length: {len(test_document)} characters")
    print(f"Number of chunks: {len(chunks)}")
    print()
    
    # Simulate context retrieval
    query = "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ"  # "What is machine learning?" in Chinese
    
    print(f"Query: '{query}'")
    print("Retrieved context chunks:")
    
    for i, chunk in enumerate(chunks):
        # Simple relevance check (in real implementation, this would use embeddings)
        relevance_score = 0
        if "æ©Ÿå™¨å­¸ç¿’" in chunk or "machine learning" in chunk.lower():
            relevance_score = 1
        if "å­¸ç¿’" in chunk or "learning" in chunk.lower():
            relevance_score += 0.5
        
        if relevance_score > 0:
            print(f"Chunk {i+1} (relevance: {relevance_score}):")
            print(f"  '{chunk.strip()[:100]}...'")
            print()
    
    print()


def test_token_count_accuracy():
    """Test token counting accuracy"""
    print("ğŸ§ª Testing token counting accuracy...")
    
    multi_tokenizer = MultilingualTokenizer()
    tiktoken_tokenizer = tiktoken.get_encoding("cl100k_base")
    
    test_cases = [
        "Simple English text for testing.",
        "é€™æ˜¯ä¸€å€‹ä¸­æ–‡æ¸¬è©¦æ–‡æœ¬ã€‚",
        "Mixed ä¸­æ–‡ and English text æ··åˆæ–‡æœ¬ã€‚",
        "This is a longer English text that should have more tokens than the previous examples. " * 5,
        "é€™æ˜¯ä¸€å€‹è¼ƒé•·çš„ä¸­æ–‡æ–‡æœ¬ï¼Œæ‡‰è©²æ¯”å‰é¢çš„ä¾‹å­æœ‰æ›´å¤šçš„æ¨™è¨˜ã€‚" * 5
    ]
    
    for i, text in enumerate(test_cases, 1):
        print(f"Test case {i}: '{text[:50]}...'")
        
        multi_count = multi_tokenizer.count_tokens(text)
        tiktoken_count = len(tiktoken_tokenizer.encode(text))
        
        print(f"  Multilingual count: {multi_count}")
        print(f"  Tiktoken count: {tiktoken_count}")
        print(f"  Difference: {abs(multi_count - tiktoken_count)}")
        print()


def test_cjk_script_detection():
    """Test CJK script detection"""
    print("ğŸ§ª Testing CJK script detection...")
    
    multi_tokenizer = MultilingualTokenizer()
    
    test_cases = [
        ("ç´”ä¸­æ–‡", "Chinese"),
        ("æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆ", "Japanese"),
        ("í•œêµ­ì–´ í…ŒìŠ¤íŠ¸", "Korean"),
        ("Mixed ä¸­æ–‡ and English", "Mixed"),
        ("Pure English text", "English")
    ]
    
    for text, expected in test_cases:
        is_cjk = multi_tokenizer.is_cjk_script(text)
        detected_lang = multi_tokenizer.detect_language_robust(text)
        
        print(f"Text: '{text}'")
        print(f"  Expected: {expected}")
        print(f"  Is CJK: {is_cjk}")
        print(f"  Detected: {detected_lang}")
        print()


def test_language_detection_robustness():
    """Test the robustness of language detection"""
    print("ğŸ§ª Testing language detection robustness...")
    
    multi_tokenizer = MultilingualTokenizer()
    
    test_cases = [
        ("Hello world", "en"),
        ("ä½ å¥½ä¸–ç•Œ", "zh"),
        ("ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ", "ja"),
        ("ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„", "ko"),
        ("Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…", "ar"),
        ("ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€", "ru"),
        ("à¸ªà¸§à¸±à¸ªà¸”à¸µà¸Šà¸²à¸§à¹‚à¸¥à¸", "th"),
        ("à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾", "hi"),
        ("Hello ä¸–ç•Œ", "mixed"),
        ("", "en")
    ]
    
    for text, expected in test_cases:
        detected = multi_tokenizer.detect_language_robust(text)
        correct = detected == expected
        print(f"Text: '{text}' -> Detected: {detected}, Expected: {expected}, Correct: {correct}")
    print()


def main():
    """Run all tests"""
    print("ğŸš€ Starting Multilingual Tokenizer Tests")
    print("=" * 50)
    
    try:
        test_basic_tokenizer()
        test_chinese_tokenizer()
        test_mixed_language_detection()
        test_chunking_functionality()
        test_vector_store_integration()
        test_context_retrieval_simulation()
        test_token_count_accuracy()
        test_cjk_script_detection()
        test_language_detection_robustness()
        
        print("âœ… All tests completed successfully!")
        
    except Exception as e:
        print(f"âŒ Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
